{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T11:16:56.878477Z",
     "start_time": "2024-12-20T11:16:56.847076Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T11:17:00.079225Z",
     "start_time": "2024-12-20T11:16:58.576888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Good to go!\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Please set GPU via Edit -> Notebook Settings.\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ],
   "id": "2811e4a6011744a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T11:17:10.349649Z",
     "start_time": "2024-12-20T11:17:00.486893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CategoryDataset:\n",
    "\n",
    "  def __init__(self, data_dir=\"News_Category_Dataset_v3.json\", batch_size=32):\n",
    "\n",
    "    total_dataset = []\n",
    "    # load dataset & get all unique words in the category\n",
    "    with open(data_dir) as f:\n",
    "        for data in f:\n",
    "            total_dataset.append(json.loads(data))\n",
    "    \n",
    "    # GPT2 Tokenizer to encode text\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") # vocab_size=50257\n",
    "    x = [tokenizer.encode(f\"{d['headline']} {d['short_description']}\".lower()) for d in total_dataset]\n",
    "    y = [d['category'] for d in total_dataset]\n",
    "    \n",
    "    self.cat2idx = self._get_cat2idx(set(y))\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "  \n",
    "    self.x_train = x_train\n",
    "    self.y_train = torch.tensor([self.cat2idx[cat] for cat in y_train])\n",
    "    self.x_val = x_val\n",
    "    self.y_val = torch.tensor([self.cat2idx[cat] for cat in y_val])\n",
    "\n",
    "    self.batch_size = batch_size\n",
    "    self.num_classes = len(set(self.y_train))\n",
    "      \n",
    "    print(f\"Training Samples: {len(self.x_train)}\")    \n",
    "    print(f\"Validation Samples: {len(self.x_val)}\")\n",
    "    print(f\"Categories: {len(set(self.y_train))}\")\n",
    "      \n",
    "    self.train_start_idx = 0\n",
    "    self.val_start_idx = 0\n",
    "    \n",
    "  @staticmethod   \n",
    "  def _padding(tokens, max_padding):\n",
    "    x = []\n",
    "    for token in tokens:\n",
    "        padded = torch.cat([torch.tensor(token), torch.zeros(max_padding-len(token)).fill_(50257)], dim=-1)\n",
    "        x.append(padded)\n",
    "    return torch.stack(x, dim=0)\n",
    "\n",
    "  @staticmethod\n",
    "  def _get_cat2idx(category):\n",
    "    unique_cat = list(set(category))\n",
    "    cat2idx = {}\n",
    "    for idx, cat in enumerate(unique_cat):\n",
    "      cat2idx[cat] = idx\n",
    "    return cat2idx\n",
    "\n",
    "  def get_next_batch(self):\n",
    "    self.train_start_idx += 1\n",
    "    if self.train_start_idx >= len(self.x_train):\n",
    "        self.train_start_idx = 0\n",
    "    x = torch.tensor(self.x_train[self.train_start_idx:self.train_start_idx+1]).to(torch.long)\n",
    "    y = self.y_train[self.train_start_idx:self.train_start_idx+1]\n",
    "    return x, y\n",
    "  \n",
    "  def get_next_val_batch(self):\n",
    "    \n",
    "    self.val_start_idx += 1\n",
    "    if self.val_start_idx >= len(self.x_val):\n",
    "      self.val_start_idx = 0\n",
    "    \n",
    "    x = torch.tensor(self.x_val[self.val_start_idx:self.val_start_idx+1]).to(torch.long)\n",
    "    y = self.y_val[self.val_start_idx:self.val_start_idx+1]\n",
    "\n",
    "    return x, y\n",
    "  \n",
    "ds = CategoryDataset()"
   ],
   "id": "27091c9793b9e1b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 167621\n",
      "Validation Samples: 41906\n",
      "Categories: 167621\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T11:17:10.375337Z",
     "start_time": "2024-12-20T11:17:10.351125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size=256, hidden_size=512, num_classes=42):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, input_size) \n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        emb = self.embedding(x)\n",
    "        out, hn = self.rnn(emb)\n",
    "        logits = self.fc(out.mean(dim=1))\n",
    "        \n",
    "        if y is None:\n",
    "          loss = None\n",
    "        else:\n",
    "          loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        return loss, logits"
   ],
   "id": "78587812760ca709",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T11:34:49.456817Z",
     "start_time": "2024-12-20T11:17:12.480719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 2e-4\n",
    "iters = 167621 * 10 # epochs = 10\n",
    "\n",
    "ds = CategoryDataset()\n",
    "\n",
    "model = RNNClassifier(50257, 256)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "for i, iter in enumerate(range(iters)):\n",
    "  x, y = ds.get_next_batch()\n",
    "  x = x.to(DEVICE)\n",
    "  y = y.to(DEVICE)\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  loss, _ = model(x, y)\n",
    "  if i % 10000 == 0:\n",
    "    print(f\"{iter}/{iters} || Loss: {loss}\")\n",
    "\n",
    "  loss.backward()\n",
    "  optimizer.step()"
   ],
   "id": "db7065736c2df618",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 167621\n",
      "Validation Samples: 41906\n",
      "Categories: 167621\n",
      "0/167621 || Loss: 3.6858811378479004\n",
      "10000/167621 || Loss: 0.9011767506599426\n",
      "20000/167621 || Loss: 1.8772355318069458\n",
      "30000/167621 || Loss: 4.318739891052246\n",
      "40000/167621 || Loss: 0.09421233087778091\n",
      "50000/167621 || Loss: 0.05923443287611008\n",
      "60000/167621 || Loss: 0.5570294857025146\n",
      "70000/167621 || Loss: 2.066330909729004\n",
      "80000/167621 || Loss: 3.658010959625244\n",
      "90000/167621 || Loss: 2.0739264488220215\n",
      "100000/167621 || Loss: 1.9398200511932373\n",
      "110000/167621 || Loss: 0.6870312094688416\n",
      "120000/167621 || Loss: 0.715524435043335\n",
      "130000/167621 || Loss: 0.07046086341142654\n",
      "140000/167621 || Loss: 0.5110667943954468\n",
      "150000/167621 || Loss: 1.3033519983291626\n",
      "160000/167621 || Loss: 0.32936033606529236\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T11:34:49.499507Z",
     "start_time": "2024-12-20T11:34:49.460608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "params = sum(parameter.numel() for parameter in model.parameters())\n",
    "\n",
    "print(f\"Number of Parameters: {params}\")"
   ],
   "id": "9560f1751ebb0c91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 13281578\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T07:20:11.149959Z",
     "start_time": "2024-12-20T07:19:43.320778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_val, y_val = ds.x_val, ds.y_val\n",
    "total_n = len(y_val)\n",
    "count = 0\n",
    "\n",
    "for iter in range(41906): # validation_size(41906) / batch_size(1) = 1309.xx \n",
    "  x, y = ds.get_next_val_batch()\n",
    "  x = x.to(DEVICE)\n",
    "  y = y.to(DEVICE)\n",
    "\n",
    "  _, logits = model(x)\n",
    "\n",
    "  pred = torch.argmax(logits, dim=-1)\n",
    "\n",
    "  for predict, gt in zip(pred, y):\n",
    "    if predict == gt:\n",
    "      count += 1\n",
    "\n",
    "print(f\"RNN Classifier Accuracy: {(count * 100/total_n):.2f}%\")"
   ],
   "id": "a764b5a34ba4e223",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Classifier Accuracy: 59.97%\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
