# Training Optimization Analysis with GPT-2

## ğŸŒ´ğŸŒ³ğŸŒ² Branch Overview
Navigate to the appropriate branch and run `python3 train_gpt2.py` to see what happensâ—ï¸â—ï¸â—ï¸
<p align="center">
<img width="538" alt="Screenshot 2024-10-14 at 11 52 38â€¯PM" src="https://github.com/user-attachments/assets/a27be35e-01a8-4338-b93a-d8d0d7b1dbf3">
</p>

## ğŸƒğŸ»ğŸ’¨ SpeedUp Results
The results in the table below compare the training speed improvements achieved by sequentially applying the methodologies for speed-up training shown in the graph above. 
The 'Time' column represents the time taken per epoch, measured in microseconds, while 'TPS' stands for tokens per second, indicating the number of tokens the model processes per second. 
Through seven stages of incremental optimization, a 386x improvement in training speed was achieved compared to the initial training speed.

Detailed information about default settings and all 12 training optimizations can be found [here]()
<p align="center">
![Screenshot 2024-11-23 at 4 01 05â€¯PM](https://github.com/user-attachments/assets/8d001ea5-1fd1-460f-8ed5-69000c94637f)
</p>

